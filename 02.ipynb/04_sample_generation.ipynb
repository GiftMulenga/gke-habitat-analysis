{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "Sample Generation and Data Extraction\n",
    "================================================================================\n",
    "\n",
    "Objective B: Drivers of Habitat Loss Analysis - Greater Kafue Ecosystem\n",
    "\n",
    "This script generates stratified random samples for Random Forest analysis:\n",
    "    - Creates habitat loss mask from LULC rasters\n",
    "    - Generates spatially stratified samples\n",
    "    - Extracts driver variable values\n",
    "    - Performs multicollinearity analysis\n",
    "    - Applies spatial train-test split\n",
    "\n",
    "Author: Gift Mulenga\n",
    "Institution: Copperbelt University, Zambia\n",
    "Research: MSc Thesis - Tropical Ecology\n",
    "\n",
    "Requirements:\n",
    "    - geopandas\n",
    "    - rasterio\n",
    "    - numpy\n",
    "    - pandas\n",
    "    - shapely\n",
    "    - scipy\n",
    "    - scikit-learn\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from shapely.geometry import Point\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ebdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration for sample generation.\"\"\"\n",
    "    \n",
    "    # Directory paths - UPDATE THESE\n",
    "    LULC_DIR = r\"D:/Publication/Habitat Loss in the Greater Kafue Ecosystem/data\"\n",
    "    DRIVER_DIR = r\"E:/Research/Msc_Tropical_Ecology/GKE_Objective_B/data\"\n",
    "    OUTPUT_DIR = r\"D:/Publication/Habitat Loss in the Greater Kafue Ecosystem/outputs/objective_b\"\n",
    "    \n",
    "    # Land cover class definitions\n",
    "    NATURAL_CLASSES = [2, 4, 6]  # Forest, Grassland, Water\n",
    "    DISTURBED_CLASSES = [1, 3, 5]  # Built-up, Cropland, Bareland\n",
    "    \n",
    "    CLASS_NAMES = {\n",
    "        1: 'Built-up', 2: 'Forest', 3: 'Cropland',\n",
    "        4: 'Grassland', 5: 'Bareland', 6: 'Water'\n",
    "    }\n",
    "    \n",
    "    # Sampling parameters\n",
    "    TOTAL_SAMPLES = 10000\n",
    "    LOSS_SAMPLES = 5000\n",
    "    NO_LOSS_SAMPLES = 5000\n",
    "    MIN_DISTANCE = 500  # meters\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.20\n",
    "    \n",
    "    # Spatial blocking\n",
    "    USE_SPATIAL_BLOCKING = True\n",
    "    N_SPATIAL_BLOCKS = 25\n",
    "    \n",
    "    # Driver variable files\n",
    "    DRIVER_FILES = {\n",
    "        'dist_roads': 'proximity/dist_roads.tif',\n",
    "        'dist_settlements': 'proximity/dist_settlements.tif',\n",
    "        'dist_rivers': 'proximity/dist_rivers.tif',\n",
    "        'dist_knp': 'proximity/dist_knp.tif',\n",
    "        'pop_density': 'socioeconomic/pop_density.tif',\n",
    "        'pop_change': 'socioeconomic/pop_change.tif',\n",
    "        'pct_cultivated': 'socioeconomic/pct_cultivated.tif',\n",
    "        'protection_status': 'conservation/protection_status.tif',\n",
    "        'years_protected': 'conservation/years_protected.tif',\n",
    "        'elevation': 'topographic/elevation.tif',\n",
    "        'slope': 'topographic/slope.tif',\n",
    "        'aspect': 'topographic/aspect.tif',\n",
    "        'twi': 'topographic/twi.tif',\n",
    "        'mean_rainfall': 'climatic/mean_rainfall.tif',\n",
    "        'mean_temp': 'climatic/mean_temp.tif',\n",
    "    }\n",
    "    \n",
    "    # Variables to exclude (VIF analysis)\n",
    "    VARS_TO_DROP_VIF = ['years_protected']\n",
    "    \n",
    "    # Variable categories\n",
    "    VARIABLE_CATEGORIES = {\n",
    "        'dist_roads': 'Proximity', 'dist_settlements': 'Proximity',\n",
    "        'dist_rivers': 'Proximity', 'dist_knp': 'Proximity',\n",
    "        'pop_density': 'Socio-economic', 'pop_change': 'Socio-economic',\n",
    "        'pct_cultivated': 'Socio-economic', 'protection_status': 'Conservation',\n",
    "        'years_protected': 'Conservation', 'elevation': 'Topographic',\n",
    "        'slope': 'Topographic', 'aspect': 'Topographic', 'twi': 'Topographic',\n",
    "        'mean_rainfall': 'Climatic', 'mean_temp': 'Climatic',\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_output_directories():\n",
    "    \"\"\"Create output directory structure.\"\"\"\n",
    "    dirs = [\n",
    "        Config.OUTPUT_DIR,\n",
    "        os.path.join(Config.OUTPUT_DIR, 'samples'),\n",
    "        os.path.join(Config.OUTPUT_DIR, 'metadata'),\n",
    "        os.path.join(Config.OUTPUT_DIR, 'figures'),\n",
    "        os.path.join(Config.OUTPUT_DIR, 'tables')\n",
    "    ]\n",
    "    for d in dirs:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    print(\"✓ Output directories ready\")\n",
    "\n",
    "\n",
    "def align_raster_to_reference(source_path, reference_path, output_path):\n",
    "    \"\"\"Align a raster to a reference raster's extent and resolution.\"\"\"\n",
    "    with rasterio.open(reference_path) as ref:\n",
    "        ref_transform, ref_crs, ref_shape = ref.transform, ref.crs, ref.shape\n",
    "        \n",
    "    with rasterio.open(source_path) as src:\n",
    "        dst_array = np.empty(ref_shape, dtype=src.dtypes[0])\n",
    "        reproject(\n",
    "            source=rasterio.band(src, 1),\n",
    "            destination=dst_array,\n",
    "            src_transform=src.transform,\n",
    "            src_crs=src.crs,\n",
    "            dst_transform=ref_transform,\n",
    "            dst_crs=ref_crs,\n",
    "            resampling=Resampling.nearest\n",
    "        )\n",
    "        \n",
    "        profile = src.profile.copy()\n",
    "        profile.update({\n",
    "            'height': ref_shape[0],\n",
    "            'width': ref_shape[1],\n",
    "            'transform': ref_transform,\n",
    "            'crs': ref_crs\n",
    "        })\n",
    "        \n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(dst_array, 1)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd890de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HABITAT LOSS MASK\n",
    "# =============================================================================\n",
    "\n",
    "def create_habitat_loss_mask():\n",
    "    \"\"\"\n",
    "    Create binary habitat loss mask from LULC rasters.\n",
    "    \n",
    "    Loss defined as: Natural habitat in 1984 → Disturbed in 2024\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mask data, profile, and statistics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CREATING HABITAT LOSS MASK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    lulc_1984_path = os.path.join(Config.LULC_DIR, \"GKE_1984.tif\")\n",
    "    lulc_2024_path = os.path.join(Config.LULC_DIR, \"GKE_2024.tif\")\n",
    "    \n",
    "    with rasterio.open(lulc_1984_path) as src_1984:\n",
    "        lulc_1984 = src_1984.read(1)\n",
    "        profile = src_1984.profile.copy()\n",
    "        transform = src_1984.transform\n",
    "        crs = src_1984.crs\n",
    "        res = src_1984.res[0]\n",
    "        \n",
    "    with rasterio.open(lulc_2024_path) as src_2024:\n",
    "        lulc_2024 = src_2024.read(1)\n",
    "    \n",
    "    # Align if needed\n",
    "    if lulc_1984.shape != lulc_2024.shape:\n",
    "        print(f\"  ⚠ Dimension mismatch - aligning...\")\n",
    "        aligned_path = os.path.join(Config.OUTPUT_DIR, \"GKE_2024_aligned.tif\")\n",
    "        align_raster_to_reference(lulc_2024_path, lulc_1984_path, aligned_path)\n",
    "        with rasterio.open(aligned_path) as src:\n",
    "            lulc_2024 = src.read(1)\n",
    "    \n",
    "    print(f\"\\n  Natural classes: {Config.NATURAL_CLASSES}\")\n",
    "    print(f\"  Disturbed classes: {Config.DISTURBED_CLASSES}\")\n",
    "    \n",
    "    # Create masks\n",
    "    natural_1984 = np.isin(lulc_1984, Config.NATURAL_CLASSES)\n",
    "    disturbed_2024 = np.isin(lulc_2024, Config.DISTURBED_CLASSES)\n",
    "    \n",
    "    # Loss mask: Natural in 1984 AND Disturbed in 2024\n",
    "    loss_mask = natural_1984 & disturbed_2024\n",
    "    \n",
    "    # No-loss mask: Natural in 1984 AND Natural in 2024\n",
    "    natural_2024 = np.isin(lulc_2024, Config.NATURAL_CLASSES)\n",
    "    no_loss_mask = natural_1984 & natural_2024\n",
    "    \n",
    "    # Statistics\n",
    "    n_natural_1984 = np.sum(natural_1984)\n",
    "    n_loss = np.sum(loss_mask)\n",
    "    n_no_loss = np.sum(no_loss_mask)\n",
    "    loss_rate = n_loss / n_natural_1984 * 100 if n_natural_1984 > 0 else 0\n",
    "    \n",
    "    print(f\"\\n  Natural pixels 1984: {n_natural_1984:,}\")\n",
    "    print(f\"  Loss pixels: {n_loss:,}\")\n",
    "    print(f\"  No-loss pixels: {n_no_loss:,}\")\n",
    "    print(f\"  Loss rate: {loss_rate:.2f}%\")\n",
    "    \n",
    "    # Save mask\n",
    "    profile.update(dtype=np.uint8, count=1)\n",
    "    mask_path = os.path.join(Config.OUTPUT_DIR, \"habitat_loss_mask.tif\")\n",
    "    with rasterio.open(mask_path, 'w', **profile) as dst:\n",
    "        dst.write(loss_mask.astype(np.uint8), 1)\n",
    "    print(f\"  ✓ Loss mask saved: {mask_path}\")\n",
    "    \n",
    "    return {\n",
    "        'loss_mask': loss_mask,\n",
    "        'no_loss_mask': no_loss_mask,\n",
    "        'profile': profile,\n",
    "        'transform': transform,\n",
    "        'crs': crs,\n",
    "        'resolution': res,\n",
    "        'shape': lulc_1984.shape,\n",
    "        'stats': {\n",
    "            'n_natural_1984': n_natural_1984,\n",
    "            'n_loss': n_loss,\n",
    "            'n_no_loss': n_no_loss,\n",
    "            'loss_rate': loss_rate\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9410ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAMPLE GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_stratified_samples(mask_data):\n",
    "    \"\"\"\n",
    "    Generate stratified random samples with minimum distance constraint.\n",
    "    \n",
    "    Args:\n",
    "        mask_data: Dictionary from create_habitat_loss_mask\n",
    "        \n",
    "    Returns:\n",
    "        dict: Sample coordinates, labels, and metadata\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING STRATIFIED RANDOM SAMPLES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    loss_mask = mask_data['loss_mask']\n",
    "    no_loss_mask = mask_data['no_loss_mask']\n",
    "    transform = mask_data['transform']\n",
    "    \n",
    "    np.random.seed(Config.RANDOM_STATE)\n",
    "    \n",
    "    # Get indices for each class\n",
    "    loss_indices = np.where(loss_mask)\n",
    "    no_loss_indices = np.where(no_loss_mask)\n",
    "    \n",
    "    # Random sample indices\n",
    "    n_loss = min(Config.LOSS_SAMPLES, len(loss_indices[0]))\n",
    "    n_no_loss = min(Config.NO_LOSS_SAMPLES, len(no_loss_indices[0]))\n",
    "    \n",
    "    loss_sample_idx = np.random.choice(len(loss_indices[0]), n_loss, replace=False)\n",
    "    no_loss_sample_idx = np.random.choice(len(no_loss_indices[0]), n_no_loss, replace=False)\n",
    "    \n",
    "    # Get coordinates\n",
    "    loss_rows = loss_indices[0][loss_sample_idx]\n",
    "    loss_cols = loss_indices[1][loss_sample_idx]\n",
    "    no_loss_rows = no_loss_indices[0][no_loss_sample_idx]\n",
    "    no_loss_cols = no_loss_indices[1][no_loss_sample_idx]\n",
    "    \n",
    "    # Combine\n",
    "    all_rows = np.concatenate([loss_rows, no_loss_rows])\n",
    "    all_cols = np.concatenate([loss_cols, no_loss_cols])\n",
    "    all_labels = np.concatenate([np.ones(n_loss), np.zeros(n_no_loss)])\n",
    "    \n",
    "    # Convert to coordinates\n",
    "    xs = transform[2] + all_cols * transform[0] + transform[0] / 2\n",
    "    ys = transform[5] + all_rows * transform[4] + transform[4] / 2\n",
    "    \n",
    "    # Apply minimum distance filter\n",
    "    print(f\"\\n--- Applying {Config.MIN_DISTANCE}m Minimum Distance Filter ---\")\n",
    "    coords = np.column_stack([xs, ys])\n",
    "    \n",
    "    selected = [0]\n",
    "    tree = cKDTree(coords[selected])\n",
    "    \n",
    "    for i in range(1, len(coords)):\n",
    "        dist, _ = tree.query(coords[i])\n",
    "        if dist >= Config.MIN_DISTANCE:\n",
    "            selected.append(i)\n",
    "            tree = cKDTree(coords[selected])\n",
    "    \n",
    "    print(f\"  Before: {len(coords)}, After: {len(selected)}\")\n",
    "    \n",
    "    return {\n",
    "        'x': xs[selected],\n",
    "        'y': ys[selected],\n",
    "        'row': all_rows[selected],\n",
    "        'col': all_cols[selected],\n",
    "        'habitat_loss': all_labels[selected].astype(int),\n",
    "        'n_samples': len(selected)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DRIVER VALUE EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "def extract_driver_values(sample_data):\n",
    "    \"\"\"\n",
    "    Extract driver variable values at sample locations.\n",
    "    \n",
    "    Args:\n",
    "        sample_data: Dictionary from generate_stratified_samples\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Samples with all driver values\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXTRACTING DRIVER VALUES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'x': sample_data['x'],\n",
    "        'y': sample_data['y'],\n",
    "        'row': sample_data['row'],\n",
    "        'col': sample_data['col'],\n",
    "        'habitat_loss': sample_data['habitat_loss']\n",
    "    })\n",
    "    \n",
    "    for var_name, rel_path in Config.DRIVER_FILES.items():\n",
    "        raster_path = os.path.join(Config.DRIVER_DIR, rel_path)\n",
    "        \n",
    "        if not os.path.exists(raster_path):\n",
    "            print(f\"  ⚠ {var_name}: File not found\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with rasterio.open(raster_path) as src:\n",
    "                values = []\n",
    "                for x, y in zip(df['x'], df['y']):\n",
    "                    row, col = src.index(x, y)\n",
    "                    if 0 <= row < src.height and 0 <= col < src.width:\n",
    "                        val = src.read(1)[row, col]\n",
    "                        values.append(val if val != src.nodata else np.nan)\n",
    "                    else:\n",
    "                        values.append(np.nan)\n",
    "                \n",
    "                df[var_name] = values\n",
    "                valid = df[var_name].notna().sum()\n",
    "                print(f\"  ✓ {var_name}: {valid}/{len(df)} valid\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ {var_name}: {e}\")\n",
    "    \n",
    "    # Remove rows with NaN\n",
    "    n_before = len(df)\n",
    "    df = df.dropna()\n",
    "    n_after = len(df)\n",
    "    print(f\"\\n  Removed {n_before - n_after} rows with NoData\")\n",
    "    print(f\"  Final samples: {n_after}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566557ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MULTICOLLINEARITY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_vif(df, features):\n",
    "    \"\"\"\n",
    "    Calculate Variance Inflation Factor for each feature.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with features\n",
    "        features: List of feature names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: VIF values for each feature\n",
    "    \"\"\"\n",
    "    vif_data = []\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        X = df[features].drop(columns=[feature]).values\n",
    "        y = df[feature].values\n",
    "        \n",
    "        if np.std(y) < 1e-10:\n",
    "            vif_data.append({'Variable': feature, 'VIF': np.inf})\n",
    "            continue\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        r2 = model.score(X, y)\n",
    "        \n",
    "        vif = 1 / (1 - r2) if r2 < 1 else np.inf\n",
    "        vif_data.append({'Variable': feature, 'VIF': vif})\n",
    "    \n",
    "    return pd.DataFrame(vif_data).sort_values('VIF', ascending=False)\n",
    "\n",
    "\n",
    "def analyze_multicollinearity(df, features):\n",
    "    \"\"\"\n",
    "    Analyze multicollinearity using correlation and VIF.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with features\n",
    "        features: List of feature names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: VIF analysis results\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Multicollinearity Analysis ---\")\n",
    "    \n",
    "    # Correlation analysis\n",
    "    corr_matrix = df[features].corr()\n",
    "    \n",
    "    print(\"\\n  Highly correlated pairs (|r| > 0.7):\")\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i+1, len(features)):\n",
    "            corr = corr_matrix.iloc[i, j]\n",
    "            if abs(corr) > 0.7:\n",
    "                high_corr_pairs.append((features[i], features[j], corr))\n",
    "                print(f\"    {features[i]} - {features[j]}: r = {corr:.2f}\")\n",
    "    \n",
    "    if not high_corr_pairs:\n",
    "        print(\"    None found\")\n",
    "    \n",
    "    # VIF analysis\n",
    "    vif_df = calculate_vif(df, features)\n",
    "    \n",
    "    print(\"\\n  Variance Inflation Factors:\")\n",
    "    for _, row in vif_df.iterrows():\n",
    "        print(f\"    {row['Variable']}: {row['VIF']:.2f}\")\n",
    "    \n",
    "    print(f\"\\n  Variables to drop: {Config.VARS_TO_DROP_VIF}\")\n",
    "    \n",
    "    return vif_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467641a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN-TEST SPLIT\n",
    "# =============================================================================\n",
    "\n",
    "def spatial_train_test_split(df, test_size, n_blocks, random_state):\n",
    "    \"\"\"\n",
    "    Perform spatially-blocked train-test split.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with x, y coordinates\n",
    "        test_size: Proportion for test set\n",
    "        n_blocks: Number of spatial blocks\n",
    "        random_state: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_df, test_df)\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Spatial Train-Test Split ---\")\n",
    "    \n",
    "    # Create spatial blocks\n",
    "    x_bins = pd.cut(df['x'], bins=int(np.sqrt(n_blocks)), labels=False)\n",
    "    y_bins = pd.cut(df['y'], bins=int(np.sqrt(n_blocks)), labels=False)\n",
    "    df['spatial_block'] = x_bins * int(np.sqrt(n_blocks)) + y_bins\n",
    "    \n",
    "    # Get unique blocks\n",
    "    unique_blocks = df['spatial_block'].unique()\n",
    "    n_test_blocks = max(1, int(len(unique_blocks) * test_size))\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    test_blocks = np.random.choice(unique_blocks, n_test_blocks, replace=False)\n",
    "    \n",
    "    train_df = df[~df['spatial_block'].isin(test_blocks)].copy()\n",
    "    test_df = df[df['spatial_block'].isin(test_blocks)].copy()\n",
    "    \n",
    "    print(f\"  Spatial blocks: {len(unique_blocks)}\")\n",
    "    print(f\"  Test blocks: {len(test_blocks)}\")\n",
    "    print(f\"  Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def standard_train_test_split(df, test_size, random_state):\n",
    "    \"\"\"Perform standard random train-test split.\"\"\"\n",
    "    print(\"\\n--- Standard Train-Test Split ---\")\n",
    "    \n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=test_size, random_state=random_state,\n",
    "        stratify=df['habitat_loss']\n",
    "    )\n",
    "    \n",
    "    print(f\"  Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f35624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OUTPUT FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def save_outputs(df, train_df, test_df, features, vif_df, sample_data):\n",
    "    \"\"\"Save all outputs.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAVING OUTPUTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sample points shapefile\n",
    "    geometry = [Point(xy) for xy in zip(df['x'], df['y'])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:32735\")\n",
    "    gdf.to_file(os.path.join(Config.OUTPUT_DIR, 'samples', 'sample_points.shp'))\n",
    "    print(\"  ✓ Sample points shapefile saved\")\n",
    "    \n",
    "    # Train/test CSVs\n",
    "    train_df.to_csv(os.path.join(Config.OUTPUT_DIR, 'samples', 'train_data.csv'), index=False)\n",
    "    test_df.to_csv(os.path.join(Config.OUTPUT_DIR, 'samples', 'test_data.csv'), index=False)\n",
    "    print(\"  ✓ Train/test CSVs saved\")\n",
    "    \n",
    "    # VIF results\n",
    "    vif_df.to_csv(os.path.join(Config.OUTPUT_DIR, 'tables', 'vif_analysis.csv'), index=False)\n",
    "    \n",
    "    # Metadata\n",
    "    metadata = {\n",
    "        'total_samples': len(df),\n",
    "        'train_samples': len(train_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'features': features,\n",
    "        'dropped_vars': Config.VARS_TO_DROP_VIF,\n",
    "        'spatial_blocking': Config.USE_SPATIAL_BLOCKING,\n",
    "        'random_state': Config.RANDOM_STATE,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(Config.OUTPUT_DIR, 'metadata', 'sampling_metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(\"  ✓ Metadata saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037de850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"SAMPLE GENERATION\")\n",
    "    print(\"Greater Kafue Ecosystem - Habitat Loss Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  - Natural classes: {Config.NATURAL_CLASSES}\")\n",
    "    print(f\"  - Spatial blocking: {Config.USE_SPATIAL_BLOCKING}\")\n",
    "    print(f\"  - Variables to drop (VIF): {Config.VARS_TO_DROP_VIF}\")\n",
    "    \n",
    "    create_output_directories()\n",
    "    \n",
    "    # Create loss mask\n",
    "    mask_data = create_habitat_loss_mask()\n",
    "    \n",
    "    # Generate samples\n",
    "    sample_data = generate_stratified_samples(mask_data)\n",
    "    \n",
    "    # Extract driver values\n",
    "    df = extract_driver_values(sample_data)\n",
    "    \n",
    "    # Define features\n",
    "    all_features = [f for f in Config.DRIVER_FILES.keys() if f in df.columns]\n",
    "    \n",
    "    # VIF analysis\n",
    "    vif_df = analyze_multicollinearity(df, all_features)\n",
    "    \n",
    "    # Drop high-VIF variables\n",
    "    features = [f for f in all_features if f not in Config.VARS_TO_DROP_VIF]\n",
    "    print(f\"\\n  Features after VIF filtering: {len(features)}\")\n",
    "    \n",
    "    # Train-test split\n",
    "    if Config.USE_SPATIAL_BLOCKING:\n",
    "        train_df, test_df = spatial_train_test_split(\n",
    "            df, Config.TEST_SIZE, Config.N_SPATIAL_BLOCKS, Config.RANDOM_STATE\n",
    "        )\n",
    "    else:\n",
    "        train_df, test_df = standard_train_test_split(\n",
    "            df, Config.TEST_SIZE, Config.RANDOM_STATE\n",
    "        )\n",
    "    \n",
    "    # Save outputs\n",
    "    save_outputs(df, train_df, test_df, features, vif_df, sample_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAMPLE GENERATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Total samples: {len(df)}\")\n",
    "    print(f\"  Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "    print(f\"  Features: {len(features)} (dropped: {Config.VARS_TO_DROP_VIF})\")\n",
    "    \n",
    "    return {'df': df, 'train_df': train_df, 'test_df': test_df, 'features': features}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
