{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c48849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "Random Forest Driver Analysis\n",
    "================================================================================\n",
    "\n",
    "Objective B: Drivers of Habitat Loss Analysis - Greater Kafue Ecosystem\n",
    "\n",
    "This script performs Random Forest classification to identify key drivers:\n",
    "    - Feature selection with Recursive Feature Elimination (RFE)\n",
    "    - Spatial cross-validation for robust model evaluation\n",
    "    - Variable importance analysis (Gini and Permutation)\n",
    "    - Critical threshold identification using Youden's J statistic\n",
    "    - Partial dependence analysis for driver relationships\n",
    "\n",
    "Author: Gift Mulenga\n",
    "Institution: Copperbelt University, Zambia\n",
    "Research: MSc Thesis - Tropical Ecology\n",
    "\n",
    "Requirements:\n",
    "    - scikit-learn\n",
    "    - pandas\n",
    "    - numpy\n",
    "    - matplotlib\n",
    "    - seaborn\n",
    "    - joblib\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, GroupKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, auc, confusion_matrix, cohen_kappa_score\n",
    ")\n",
    "from sklearn.tree import export_text, plot_tree\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set publication-quality plot defaults\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5234b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration for Random Forest analysis.\"\"\"\n",
    "    \n",
    "    # Directory paths - UPDATE THESE\n",
    "    BASE_DIR = r\"D:/Publication/Habitat Loss in the Greater Kafue Ecosystem/outputs/objective_b\"\n",
    "    INPUT_DIR = os.path.join(BASE_DIR, \"samples\")\n",
    "    OUTPUT_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "    TABLES_DIR = os.path.join(BASE_DIR, \"tables\")\n",
    "    FIGURES_DIR = os.path.join(BASE_DIR, \"figures\")\n",
    "    \n",
    "    # Model parameters\n",
    "    N_ESTIMATORS = 500\n",
    "    MAX_DEPTH = None\n",
    "    MIN_SAMPLES_SPLIT = 10\n",
    "    MIN_SAMPLES_LEAF = 5\n",
    "    MAX_FEATURES = 'sqrt'\n",
    "    CLASS_WEIGHT = 'balanced'\n",
    "    RANDOM_STATE = 42\n",
    "    N_JOBS = -1\n",
    "    \n",
    "    # Cross-validation\n",
    "    CV_FOLDS = 10\n",
    "    USE_SPATIAL_CV = True\n",
    "    \n",
    "    # Feature selection\n",
    "    USE_RFE = True\n",
    "    RFE_N_FEATURES = 10\n",
    "    \n",
    "    # Variables to exclude\n",
    "    VARS_TO_EXCLUDE = ['years_protected']\n",
    "    \n",
    "    # Variable categories\n",
    "    VARIABLE_CATEGORIES = {\n",
    "        'dist_roads': 'Proximity', 'dist_settlements': 'Proximity',\n",
    "        'dist_rivers': 'Proximity', 'dist_knp': 'Proximity',\n",
    "        'pop_density': 'Socio-economic', 'pop_change': 'Socio-economic',\n",
    "        'pct_cultivated': 'Socio-economic', 'protection_status': 'Conservation',\n",
    "        'elevation': 'Topographic', 'slope': 'Topographic',\n",
    "        'aspect': 'Topographic', 'twi': 'Topographic',\n",
    "        'mean_rainfall': 'Climatic', 'mean_temp': 'Climatic',\n",
    "    }\n",
    "    \n",
    "    # Display names\n",
    "    VARIABLE_DISPLAY_NAMES = {\n",
    "        'dist_roads': 'Distance to Roads',\n",
    "        'dist_settlements': 'Distance to Settlements',\n",
    "        'dist_rivers': 'Distance to Rivers',\n",
    "        'dist_knp': 'Distance to KNP',\n",
    "        'pop_density': 'Population Density',\n",
    "        'pop_change': 'Population Change',\n",
    "        'pct_cultivated': 'Percent Cultivated',\n",
    "        'protection_status': 'Protection Status',\n",
    "        'elevation': 'Elevation',\n",
    "        'slope': 'Slope',\n",
    "        'aspect': 'Aspect',\n",
    "        'twi': 'Topographic Wetness Index',\n",
    "        'mean_rainfall': 'Mean Rainfall',\n",
    "        'mean_temp': 'Mean Temperature',\n",
    "    }\n",
    "    \n",
    "    # Category colors\n",
    "    CATEGORY_COLORS = {\n",
    "        'Socio-economic': '#E74C3C',\n",
    "        'Climatic': '#3498DB',\n",
    "        'Proximity': '#2ECC71',\n",
    "        'Topographic': '#9B59B6',\n",
    "        'Conservation': '#F39C12',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b232fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def create_output_directories():\n",
    "    \"\"\"Create output directories.\"\"\"\n",
    "    for d in [Config.OUTPUT_DIR, Config.TABLES_DIR, Config.FIGURES_DIR]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    print(\"✓ Output directories ready\")\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load training and test data.\"\"\"\n",
    "    print(\"\\n--- Loading Data ---\")\n",
    "    \n",
    "    train_df = pd.read_csv(os.path.join(Config.INPUT_DIR, 'train_data.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(Config.INPUT_DIR, 'test_data.csv'))\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_df)}\")\n",
    "    print(f\"  Test samples: {len(test_df)}\")\n",
    "    \n",
    "    # Identify features\n",
    "    exclude_cols = ['x', 'y', 'habitat_loss', 'row', 'col', 'spatial_block'] + Config.VARS_TO_EXCLUDE\n",
    "    features = [c for c in train_df.columns if c not in exclude_cols]\n",
    "    \n",
    "    print(f\"  Features: {len(features)}\")\n",
    "    print(f\"  Excluded: {Config.VARS_TO_EXCLUDE}\")\n",
    "    \n",
    "    return train_df, test_df, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a62370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE SELECTION\n",
    "# =============================================================================\n",
    "\n",
    "def perform_rfe(X_train, y_train, features, n_features_to_select=10):\n",
    "    \"\"\"\n",
    "    Perform Recursive Feature Elimination.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        features: Feature names\n",
    "        n_features_to_select: Number of features to select\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (selected features, ranking DataFrame)\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Recursive Feature Elimination ---\")\n",
    "    \n",
    "    rf_rfe = RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=10,\n",
    "        random_state=Config.RANDOM_STATE, n_jobs=Config.N_JOBS\n",
    "    )\n",
    "    \n",
    "    rfe = RFE(rf_rfe, n_features_to_select=n_features_to_select, step=1)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    ranking_df = pd.DataFrame({\n",
    "        'Variable': features,\n",
    "        'RFE_Rank': rfe.ranking_,\n",
    "        'Selected': rfe.support_\n",
    "    }).sort_values('RFE_Rank')\n",
    "    \n",
    "    selected_features = ranking_df[ranking_df['Selected']]['Variable'].tolist()\n",
    "    \n",
    "    print(f\"  Selected {len(selected_features)} features:\")\n",
    "    for f in selected_features:\n",
    "        print(f\"    - {f}\")\n",
    "    \n",
    "    return selected_features, ranking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train Random Forest classifier.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Training Random Forest ---\")\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=Config.N_ESTIMATORS,\n",
    "        max_depth=Config.MAX_DEPTH,\n",
    "        min_samples_split=Config.MIN_SAMPLES_SPLIT,\n",
    "        min_samples_leaf=Config.MIN_SAMPLES_LEAF,\n",
    "        max_features=Config.MAX_FEATURES,\n",
    "        class_weight=Config.CLASS_WEIGHT,\n",
    "        random_state=Config.RANDOM_STATE,\n",
    "        n_jobs=Config.N_JOBS,\n",
    "        oob_score=True\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"  OOB Score: {model.oob_score_:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CROSS-VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def spatial_cross_validation(model, X, y, groups, n_folds):\n",
    "    \"\"\"\n",
    "    Perform spatial (grouped) cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        model: Classifier\n",
    "        X: Features\n",
    "        y: Labels\n",
    "        groups: Spatial block assignments\n",
    "        n_folds: Number of folds\n",
    "        \n",
    "    Returns:\n",
    "        dict: Cross-validation scores\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Spatial Cross-Validation ---\")\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=min(n_folds, len(np.unique(groups))))\n",
    "    \n",
    "    scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    cv_results = cross_validate(model, X, y, groups=groups, cv=gkf, scoring=scoring)\n",
    "    \n",
    "    scores = {\n",
    "        'accuracy': cv_results['test_accuracy'],\n",
    "        'precision': cv_results['test_precision'],\n",
    "        'recall': cv_results['test_recall'],\n",
    "        'f1': cv_results['test_f1'],\n",
    "        'roc_auc': cv_results['test_roc_auc']\n",
    "    }\n",
    "    \n",
    "    print(\"\\n  CV Results (mean ± std):\")\n",
    "    for metric, values in scores.items():\n",
    "        print(f\"    {metric}: {np.mean(values):.4f} ± {np.std(values):.4f}\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def standard_cross_validation(model, X, y, n_folds):\n",
    "    \"\"\"Perform standard stratified cross-validation.\"\"\"\n",
    "    print(\"\\n--- Standard Cross-Validation ---\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=Config.RANDOM_STATE)\n",
    "    \n",
    "    scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    cv_results = cross_validate(model, X, y, cv=skf, scoring=scoring)\n",
    "    \n",
    "    scores = {\n",
    "        'accuracy': cv_results['test_accuracy'],\n",
    "        'precision': cv_results['test_precision'],\n",
    "        'recall': cv_results['test_recall'],\n",
    "        'f1': cv_results['test_f1'],\n",
    "        'roc_auc': cv_results['test_roc_auc']\n",
    "    }\n",
    "    \n",
    "    print(\"\\n  CV Results (mean ± std):\")\n",
    "    for metric, values in scores.items():\n",
    "        print(f\"    {metric}: {np.mean(values):.4f} ± {np.std(values):.4f}\")\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, features):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        X_test: Test features\n",
    "        y_test: Test labels\n",
    "        features: Feature names\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (metrics dict, confusion matrix)\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Model Evaluation ---\")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "        'kappa': cohen_kappa_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n  Test Set Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"    {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Confusion Matrix:\")\n",
    "    print(f\"    TN: {cm[0, 0]:,}  FP: {cm[0, 1]:,}\")\n",
    "    print(f\"    FN: {cm[1, 0]:,}  TP: {cm[1, 1]:,}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    metrics_df.to_csv(os.path.join(Config.TABLES_DIR, 'Table_Model_Performance.csv'), index=False)\n",
    "    print(\"\\n  ✓ Performance metrics saved\")\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {metrics[\"roc_auc\"]:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.FIGURES_DIR, 'Figure_ROC_Curve.png'), dpi=300)\n",
    "    plt.close()\n",
    "    print(\"  ✓ ROC curve saved\")\n",
    "    \n",
    "    return metrics, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ecba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VARIABLE IMPORTANCE\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_importance(model, X_train, y_train, X_test, y_test, features):\n",
    "    \"\"\"\n",
    "    Analyze variable importance using multiple methods.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        X_train, y_train: Training data\n",
    "        X_test, y_test: Test data\n",
    "        features: Feature names\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (importance DataFrame, category summary)\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Variable Importance Analysis ---\")\n",
    "    \n",
    "    # Gini importance\n",
    "    gini_importance = model.feature_importances_\n",
    "    \n",
    "    # Permutation importance\n",
    "    perm_result = permutation_importance(\n",
    "        model, X_test, y_test,\n",
    "        n_repeats=30, random_state=Config.RANDOM_STATE, n_jobs=Config.N_JOBS\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Variable': features,\n",
    "        'Gini_Importance': gini_importance,\n",
    "        'Permutation_Importance': perm_result.importances_mean,\n",
    "        'Permutation_Std': perm_result.importances_std\n",
    "    })\n",
    "    \n",
    "    # Add ranks\n",
    "    importance_df['Gini_Rank'] = importance_df['Gini_Importance'].rank(ascending=False).astype(int)\n",
    "    importance_df['Permutation_Rank'] = importance_df['Permutation_Importance'].rank(ascending=False).astype(int)\n",
    "    \n",
    "    # Add categories\n",
    "    importance_df['Category'] = importance_df['Variable'].map(Config.VARIABLE_CATEGORIES)\n",
    "    importance_df['Display_Name'] = importance_df['Variable'].map(Config.VARIABLE_DISPLAY_NAMES)\n",
    "    \n",
    "    # Sort by Gini importance\n",
    "    importance_df = importance_df.sort_values('Gini_Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n  Top 10 Variables (Gini):\")\n",
    "    for i, row in importance_df.head(10).iterrows():\n",
    "        print(f\"    {row['Gini_Rank']:2d}. {row['Display_Name']}: {row['Gini_Importance']:.4f}\")\n",
    "    \n",
    "    # Save\n",
    "    importance_df.to_csv(os.path.join(Config.TABLES_DIR, 'Table_Variable_Importance.csv'), index=False)\n",
    "    print(\"\\n  ✓ Variable importance table saved\")\n",
    "    \n",
    "    # Category summary\n",
    "    category_summary = importance_df.groupby('Category').agg({\n",
    "        'Gini_Importance': 'sum',\n",
    "        'Permutation_Importance': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    category_summary['Gini_Pct'] = category_summary['Gini_Importance'] / category_summary['Gini_Importance'].sum() * 100\n",
    "    category_summary = category_summary.sort_values('Gini_Importance', ascending=False)\n",
    "    \n",
    "    category_summary.to_csv(os.path.join(Config.TABLES_DIR, 'Table_Category_Importance.csv'), index=False)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Bar chart\n",
    "    colors = [Config.CATEGORY_COLORS.get(c, 'gray') for c in importance_df['Category']]\n",
    "    axes[0].barh(importance_df['Display_Name'].head(10)[::-1], \n",
    "                 importance_df['Gini_Importance'].head(10)[::-1],\n",
    "                 color=colors[:10][::-1])\n",
    "    axes[0].set_xlabel('Gini Importance')\n",
    "    axes[0].set_title('Top 10 Variables')\n",
    "    \n",
    "    # Category pie\n",
    "    axes[1].pie(category_summary['Gini_Pct'], labels=category_summary['Category'],\n",
    "                autopct='%1.1f%%', colors=[Config.CATEGORY_COLORS.get(c, 'gray') \n",
    "                                           for c in category_summary['Category']])\n",
    "    axes[1].set_title('Importance by Category')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.FIGURES_DIR, 'Figure_Variable_Importance.png'), dpi=300)\n",
    "    plt.close()\n",
    "    print(\"  ✓ Variable importance figure saved\")\n",
    "    \n",
    "    return importance_df, category_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd929033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# THRESHOLD ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_thresholds(model, X_train, y_train, features):\n",
    "    \"\"\"\n",
    "    Identify critical thresholds using Youden's J statistic.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        features: Feature names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Threshold analysis results\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Critical Threshold Analysis ---\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        # Get feature values\n",
    "        X_single = X_train[:, i].reshape(-1, 1)\n",
    "        \n",
    "        # Simple logistic-style analysis\n",
    "        loss_values = X_single[y_train == 1].flatten()\n",
    "        no_loss_values = X_single[y_train == 0].flatten()\n",
    "        \n",
    "        # Find optimal threshold using ROC\n",
    "        thresholds = np.percentile(X_single, np.arange(5, 96, 5))\n",
    "        \n",
    "        best_j = 0\n",
    "        best_thresh = np.median(X_single)\n",
    "        best_sens = 0\n",
    "        best_spec = 0\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            pred = (X_single.flatten() <= thresh).astype(int)\n",
    "            tp = np.sum((pred == 1) & (y_train == 1))\n",
    "            tn = np.sum((pred == 0) & (y_train == 0))\n",
    "            fp = np.sum((pred == 1) & (y_train == 0))\n",
    "            fn = np.sum((pred == 0) & (y_train == 1))\n",
    "            \n",
    "            sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            j = sens + spec - 1\n",
    "            \n",
    "            if j > best_j:\n",
    "                best_j = j\n",
    "                best_thresh = thresh\n",
    "                best_sens = sens\n",
    "                best_spec = spec\n",
    "        \n",
    "        results.append({\n",
    "            'Variable': feature,\n",
    "            'Display_Name': Config.VARIABLE_DISPLAY_NAMES.get(feature, feature),\n",
    "            'Optimal_Threshold': best_thresh,\n",
    "            'Youden_J': best_j,\n",
    "            'Sensitivity': best_sens,\n",
    "            'Specificity': best_spec,\n",
    "            'Mean_Loss': np.mean(loss_values),\n",
    "            'Mean_NoLoss': np.mean(no_loss_values)\n",
    "        })\n",
    "    \n",
    "    threshold_df = pd.DataFrame(results).sort_values('Youden_J', ascending=False)\n",
    "    threshold_df.to_csv(os.path.join(Config.TABLES_DIR, 'Table_Critical_Thresholds.csv'), index=False)\n",
    "    \n",
    "    print(\"\\n  Top 5 Discriminating Variables:\")\n",
    "    for i, row in threshold_df.head(5).iterrows():\n",
    "        print(f\"    {row['Display_Name']}: threshold = {row['Optimal_Threshold']:.1f}, J = {row['Youden_J']:.3f}\")\n",
    "    \n",
    "    print(\"\\n  ✓ Threshold analysis saved\")\n",
    "    \n",
    "    return threshold_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARTIAL DEPENDENCE\n",
    "# =============================================================================\n",
    "\n",
    "def create_pdp(model, X_train, features):\n",
    "    \"\"\"\n",
    "    Create partial dependence plots for top variables.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        X_train: Training features\n",
    "        features: Feature names\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Partial Dependence Plots ---\")\n",
    "    \n",
    "    # Get top 6 features\n",
    "    importance = model.feature_importances_\n",
    "    top_idx = np.argsort(importance)[-6:][::-1]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
    "    \n",
    "    for idx, (ax, feat_idx) in enumerate(zip(axes.flatten(), top_idx)):\n",
    "        PartialDependenceDisplay.from_estimator(\n",
    "            model, X_train, [feat_idx],\n",
    "            ax=ax, grid_resolution=50\n",
    "        )\n",
    "        display_name = Config.VARIABLE_DISPLAY_NAMES.get(features[feat_idx], features[feat_idx])\n",
    "        ax.set_title(display_name, fontsize=11)\n",
    "        ax.set_xlabel('')\n",
    "    \n",
    "    plt.suptitle('Partial Dependence Plots', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.FIGURES_DIR, 'Figure_Partial_Dependence.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"  ✓ Partial dependence plots saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866621f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL PERSISTENCE\n",
    "# =============================================================================\n",
    "\n",
    "def save_model_and_metadata(model, features, metrics, cv_scores):\n",
    "    \"\"\"Save trained model and metadata.\"\"\"\n",
    "    print(\"\\n--- Saving Model and Metadata ---\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(Config.OUTPUT_DIR, 'rf_model.joblib')\n",
    "    joblib.dump(model, model_path)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'model_params': {\n",
    "            'n_estimators': Config.N_ESTIMATORS,\n",
    "            'max_depth': Config.MAX_DEPTH,\n",
    "            'min_samples_split': Config.MIN_SAMPLES_SPLIT,\n",
    "            'min_samples_leaf': Config.MIN_SAMPLES_LEAF,\n",
    "            'max_features': Config.MAX_FEATURES\n",
    "        },\n",
    "        'features': features,\n",
    "        'test_metrics': metrics,\n",
    "        'cv_metrics': {k: {'mean': float(np.mean(v)), 'std': float(np.std(v))} \n",
    "                       for k, v in cv_scores.items()},\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(Config.OUTPUT_DIR, 'model_metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(\"  ✓ Model and metadata saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb98631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"RANDOM FOREST DRIVER ANALYSIS\")\n",
    "    print(\"Greater Kafue Ecosystem - Habitat Loss Drivers\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  - Spatial CV: {Config.USE_SPATIAL_CV}\")\n",
    "    print(f\"  - RFE: {Config.USE_RFE}\")\n",
    "    print(f\"  - Variables excluded: {Config.VARS_TO_EXCLUDE}\")\n",
    "    \n",
    "    create_output_directories()\n",
    "    \n",
    "    # Load data\n",
    "    train_df, test_df, features = load_data()\n",
    "    \n",
    "    # Prepare arrays\n",
    "    X_train = train_df[features].values\n",
    "    y_train = train_df['habitat_loss'].values\n",
    "    X_test = test_df[features].values\n",
    "    y_test = test_df['habitat_loss'].values\n",
    "    \n",
    "    # Feature selection\n",
    "    if Config.USE_RFE:\n",
    "        selected_features, rfe_ranking = perform_rfe(X_train, y_train, features, Config.RFE_N_FEATURES)\n",
    "        rfe_ranking.to_csv(os.path.join(Config.TABLES_DIR, 'Table_RFE_Ranking.csv'), index=False)\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(X_train, y_train)\n",
    "    \n",
    "    # Cross-validation\n",
    "    if Config.USE_SPATIAL_CV and 'spatial_block' in train_df.columns:\n",
    "        groups = train_df['spatial_block'].values\n",
    "        cv_scores = spatial_cross_validation(model, X_train, y_train, groups, Config.CV_FOLDS)\n",
    "    else:\n",
    "        cv_scores = standard_cross_validation(model, X_train, y_train, Config.CV_FOLDS)\n",
    "    \n",
    "    # Evaluation\n",
    "    metrics, cm = evaluate_model(model, X_test, y_test, features)\n",
    "    \n",
    "    # Variable importance\n",
    "    importance_df, category_summary = analyze_importance(\n",
    "        model, X_train, y_train, X_test, y_test, features\n",
    "    )\n",
    "    \n",
    "    # Threshold analysis\n",
    "    threshold_df = analyze_thresholds(model, X_train, y_train, features)\n",
    "    \n",
    "    # Partial dependence\n",
    "    create_pdp(model, X_train, features)\n",
    "    \n",
    "    # Save model\n",
    "    save_model_and_metadata(model, features, metrics, cv_scores)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nOutputs saved to:\")\n",
    "    print(f\"  Tables: {Config.TABLES_DIR}\")\n",
    "    print(f\"  Figures: {Config.FIGURES_DIR}\")\n",
    "    print(f\"  Model: {Config.OUTPUT_DIR}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'features': features,\n",
    "        'metrics': metrics,\n",
    "        'importance': importance_df,\n",
    "        'cv_scores': cv_scores,\n",
    "        'thresholds': threshold_df\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
